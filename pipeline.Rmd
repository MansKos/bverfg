---
title: "BVerfG RAG Corpus Pipeline"
author: "RAG-optimierte Version"  
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

# Bibliotheken laden
library(targets)
library(tarchetypes)
library(future)
library(future.callr)
library(RcppTOML)
library(data.table)
library(rvest)
library(httr)
library(stringr)
library(dplyr)
library(jsonlite)

# Konfiguration laden
config <- RcppTOML::parseTOML("config.toml")

# Parallelisierung einrichten
if(config$cores$max) {
  plan(callr, workers = parallel::detectCores())
} else {
  plan(callr, workers = config$cores$number)
}

# Output-Ordner erstellen
dir.create("output", showWarnings = FALSE)
```

# BVerfG RAG Corpus Pipeline

Diese schlanke Pipeline extrahiert BVerfG-Entscheidungen optimal f√ºr RAG-Anwendungen.

## Funktionen laden

```{r load-functions}
# Nur essenzielle Funktionen f√ºr RAG
source("functions/f.download.R")
source("functions/f.download_table_make.R")
source("functions/f.parse_html_bverfg.R") 
source("functions/f.bverfg.extract.content.R")
source("functions/f.bverfg.extract.meta.R")
source("functions/f.clean_meta.R")
source("functions/f.readtext.R")
```

## Targets Pipeline definieren

```{r define-targets}
tar_config_set(store = "_targets")

rag_targets <- list(
  # 1. Download-Tabelle erstellen (mit Debug-Modus falls aktiviert)
  tar_target(
    name = download_table,
    command = f.download_table_make(
      debug.toggle = config$debug$toggle,
      debug.pages = config$debug$pages
    )
  ),
  
  # 2. Dateinamen f√ºr Downloads vorbereiten
  tar_target(
    name = download_prep,
    command = {
      # HTML-URLs und Dateinamen extrahieren
      urls <- download_table$url_html
      filenames <- paste0("decision_", seq_len(nrow(download_table)), ".html")
      
      list(urls = urls, filenames = filenames)
    }
  ),
  
  # 3. HTML-Dateien herunterladen (nur neue Dateien!)
  tar_target(
    name = html_files,
    command = f.download(
      url = download_prep$urls,
      filename = download_prep$filenames,
      dir = "html_cache",
      clean = FALSE,  # Keine Bereinigung, um vorhandene Dateien zu behalten
      debug.toggle = config$debug$toggle
    )
  ),
  
  # 4. HTML-Dateien parsen (nur neue/ge√§nderte Dateien)
  tar_target(
    name = parsed_decisions,
    command = {
      parsed_list <- vector("list", length(html_files))
      for(i in seq_along(html_files)) {
        if(file.exists(html_files[i])) {
          parsed_list[[i]] <- f.parse_html_bverfg(html_files[i])
        }
      }
      parsed_list[!sapply(parsed_list, is.null)]
    }
  ),
  
  # 5. Inhalte extrahieren
  tar_target(
    name = extracted_texts,
    command = {
      text_list <- lapply(parsed_decisions, f.bverfg.extract.content)
      do.call(rbind, text_list)
    }
  ),
  
  # 6. Metadaten extrahieren
  tar_target(
    name = extracted_meta,
    command = {
      meta_list <- lapply(parsed_decisions, f.bverfg.extract.meta)
      do.call(rbind, meta_list)
    }
  ),
  
  # 7. Daten kombinieren und bereinigen
  tar_target(
    name = rag_corpus,
    command = {
      # Sicherstellen dass beide data.tables sind
      if(!is.data.table(extracted_meta)) extracted_meta <- as.data.table(extracted_meta)
      if(!is.data.table(extracted_texts)) extracted_texts <- as.data.table(extracted_texts)
      
      # Zusammenf√ºhren (falls beide gleiche ID-Spalte haben)
      if("id" %in% names(extracted_meta) && "id" %in% names(extracted_texts)) {
        corpus <- merge(extracted_meta, extracted_texts, by = "id", all = TRUE)
      } else {
        # Falls keine ID-Spalte, einfach cbind (angenommen gleiche Reihenfolge)
        corpus <- cbind(extracted_meta, extracted_texts)
      }
      
      # Bereinigung f√ºr RAG
      if("content" %in% names(corpus)) {
        corpus <- corpus[!is.na(content) & nchar(content) > 100]
        corpus[, content := stringr::str_trim(content)]
      }
      
      # Falls URL-Spalte nicht vorhanden, aus download_table hinzuf√ºgen
      if(!"url" %in% names(corpus) && nrow(corpus) == nrow(download_table)) {
        corpus[, url := download_table$url_html]
      }
      
      return(corpus)
    }
  ),
  
  # 8. RAG-Export mit inkrementeller Info
  tar_target(
    name = rag_export,
    command = {
      # Timestamp f√ºr inkrementelle Updates
      timestamp <- Sys.time()
      
      # Corpus mit Metadaten erg√§nzen
      rag_corpus[, last_updated := timestamp]
      
      # Export in verschiedenen Formaten
      fwrite(rag_corpus, "output/bverfg_rag_corpus.csv")
      saveRDS(rag_corpus, "output/bverfg_rag_corpus.rds")
      jsonlite::write_json(rag_corpus, "output/bverfg_rag_corpus.json")
      
      # Inkrementelle Update-Info speichern
      update_info <- list(
        last_run = timestamp,
        total_decisions = nrow(rag_corpus),
        cache_dir = "html_cache"
      )
      saveRDS(update_info, "output/update_info.rds")
      
      # Statistiken ausgeben
      cat("üìÑ RAG Corpus aktualisiert:\n")
      cat("- Anzahl Entscheidungen:", nrow(rag_corpus), "\n")
      cat("- Letzte Aktualisierung:", as.character(timestamp), "\n")
      cat("- Cache-Ordner:", "html_cache/", "\n")
      
      if(file.exists("output/update_info.rds")) {
        cat("‚úÖ Inkrementelle Updates aktiv - nur neue Entscheidungen werden heruntergeladen\n")
      }
      
      return("Export abgeschlossen")
    }
  )
)
```

## Pipeline ausf√ºhren

```{r run-pipeline, eval=FALSE}
# Targets definieren und Pipeline starten
tar_script({
  library(targets)
  library(tarchetypes)
  library(data.table)
  library(jsonlite)
  
  # Konfiguration laden
  config <- RcppTOML::parseTOML("config.toml")
  
  # Funktionen laden
  source("functions/f.download.R")
  source("functions/f.download_table_make.R") 
  source("functions/f.linkextract.R")
  source("functions/f.parse_html_bverfg.R")
  source("functions/f.bverfg.extract.content.R")
  source("functions/f.bverfg.extract.meta.R")
  source("functions/f.clean_meta.R")
  
  rag_targets
})

# Pipeline starten
tar_make()
```

## Update-Status pr√ºfen

```{r check-updates}
if(file.exists("output/update_info.rds")) {
  update_info <- readRDS("output/update_info.rds")
  
  cat("üîÑ Letzte Aktualisierung:", as.character(update_info$last_run), "\n")
  cat("üìÅ Cache-Ordner:", update_info$cache_dir, "\n")
  
  # Anzahl gecachter Dateien
  if(dir.exists(update_info$cache_dir)) {
    cached_files <- length(list.files(update_info$cache_dir, pattern = "\\.html$"))
    cat("üíæ Gecachte HTML-Dateien:", cached_files, "\n")
  }
} else {
  cat("‚ÑπÔ∏è Noch keine Updates durchgef√ºhrt.\n")
}
```

## Ergebnisse

```{r results}
if(file.exists("output/bverfg_rag_corpus.csv")) {
  cat("‚úÖ RAG Corpus erfolgreich erstellt!\n\n")
  
  corpus <- fread("output/bverfg_rag_corpus.csv")
  
  cat("üìä Corpus-Statistiken:\n")
  cat("- Gesamtzahl Entscheidungen:", nrow(corpus), "\n")
  
  if("content" %in% names(corpus)) {
    cat("- Textl√§nge (Median):", median(nchar(corpus$content), na.rm = TRUE), "Zeichen\n")
  }
  
  if("date" %in% names(corpus)) {
    date_range <- range(corpus$date, na.rm = TRUE)
    cat("- Zeitraum:", date_range[1], "bis", date_range[2], "\n")
  }
  
  cat("\nüìÅ Verf√ºgbare Dateien:\n")
  cat("- CSV: output/bverfg_rag_corpus.csv\n")
  cat("- RDS: output/bverfg_rag_corpus.rds\n")  
  cat("- JSON: output/bverfg_rag_corpus.json\n")
  cat("- Update-Info: output/update_info.rds\n")
} else {
  cat("‚ùå Pipeline noch nicht ausgef√ºhrt. F√ºhren Sie tar_make() aus.\n")
}
```

## üîÑ Inkrementelle Updates

**So funktioniert das intelligente Download-System:**

1. **Beim ersten Lauf**: 
   - Alle BVerfG-Entscheidungen werden heruntergeladen
   - Dateien werden in `html_cache/` gespeichert

2. **Bei sp√§teren L√§ufen**:
   - Nur neue Entscheidungen werden heruntergeladen  
   - Bereits vorhandene HTML-Dateien werden √ºbersprungen
   - ‚ö° **Deutlich schneller!**

3. **Cache-Management**:
   - HTML-Dateien bleiben erhalten
   - Keine unn√∂tigen Re-Downloads
   - Update-Informationen in `output/update_info.rds`

## F√ºr RAG optimierte Datenstruktur

Die exportierten Daten enthalten:

- **id**: Eindeutige Kennung
- **title**: Titel der Entscheidung  
- **date**: Entscheidungsdatum
- **content**: Volltext (bereinigt)
- **url**: Original-URL
- **last_updated**: Zeitstempel der letzten Aktualisierung

Diese Struktur ist optimal f√ºr:
- ‚úÖ Vektorisierung mit Embedding-Modellen
- ‚úÖ Chunking f√ºr RAG-Systeme  
- ‚úÖ Metadaten-basierte Filterung
- ‚úÖ Inkrementelle Updates
```